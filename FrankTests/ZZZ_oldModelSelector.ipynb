{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESSENTIALS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CLUSTERING AND RANDOM FOREST\n",
    "import skfuzzy as fuzz\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import model_selection\n",
    "\n",
    "# DATA LIBRARIES\n",
    "import geopandas as gpd\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# PREFERENCES\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our Scikit-Learn Compliant Class so we can implement validation schemes within. Only two methods required: predict() and fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelector():\n",
    "\n",
    "    def __init__(self, c_kwargs={}, rf_kwargs={}):\n",
    "       self.c_kwargs=c_kwargs        # CLUSTERING HYPERPARAMETERS\n",
    "       self.rf_kwargs=rf_kwargs      # RANDOM FOREST HYPERPARAMETERS\n",
    "       self.m = 2                    # EXPONENTIATION COEFFICIENT FOR CLUSTERING. TODO: MAKE ADJUSTABLE\n",
    "\n",
    "    def fuzzyCluster(self, data):\n",
    "        # Wraps Fuzzy Cluster function, only outputting percent belongs and formal cluster.\n",
    "\n",
    "        # CHECK THAT REQUIRED FIELDS ARE IN KWARGS, IF NOT ADD\n",
    "        if \"error\" not in self.c_kwargs:\n",
    "            self.c_kwargs['error']=0.005\n",
    "\n",
    "        if \"maxiter\" not in self.c_kwargs:\n",
    "            self.c_kwargs['maxiter']=1000\n",
    "\n",
    "        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(data.T, self.n_centers, self.m, **self.c_kwargs)\n",
    "        label = np.argmax(u, axis=0)\n",
    "        return cntr, u, fpc, label\n",
    "\n",
    "    def howManyClusters(self, X, mintest=2,maxtest=15):\n",
    "        # Determines how many clusters should be used using the Fuzzy Partitions Coefficient (FPC)\n",
    "        # https://scikit-fuzzy.github.io/scikit-fuzzy/auto_examples/plot_cmeans.html#example-plot-cmeans-py\n",
    "        # TODO: FIGURE OUT IF THIS METHOD IS APPROPRIATE OR NOT\n",
    "        fpcs = []\n",
    "        listtests = np.arange(mintest,maxtest)\n",
    "        for ncenters in listtests:\n",
    "            self.n_centers = ncenters\n",
    "            _, _, fpc, _ = self.fuzzyCluster(X)\n",
    "            fpcs.append(fpc)\n",
    "        return listtests[np.argmax(fpcs)]\n",
    "\n",
    "    def train_rf(self, X_train, y_train, rf_controls={}):\n",
    "        # ADAPTED FROM https://stackoverflow.com/questions/28489667/combining-random-forest-models-in-scikit-learn\n",
    "\n",
    "        # RF CONTROLS PASSED DIRECTLY FROM PARAMETER, DEFAULT IS EMPTY\n",
    "        rf = RandomForestRegressor(**rf_controls) \n",
    "\n",
    "        # RF FITTING \n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        return rf\n",
    "\n",
    "    def fit(self, attributes, model_perf):\n",
    "        # GET NUMBER OF CLUSTERS VIA MAXIMUM FUZZY PARTITION COEFFICIENT \n",
    "        self.n_centers = self.howManyClusters(model_perf)\n",
    "\n",
    "        # RUN CLUSTERING AND SAVE CENTERS FOR FUTURE PREDICTIONS\n",
    "        cntr, u, fpc, label = self.fuzzyCluster(model_perf)\n",
    "        # cntr:  CLUSTER CENTERS, WHICH ARE ON N-DIM MODEL PERFORMANCE SPACE\n",
    "        #        WHERE N IS THE NUMBER OF MODELS BEING COMPARED. \n",
    "        # u:     CLUSTER MEMBERSHIP MATRIX (% BELONGING)\n",
    "        # fpc:   FUZZY PARTITION COEFFICIENT FOR CLUSTERING RUN\n",
    "        # label: \"CLOSEST CLUSTER\", DEFINED BY MAXIMUM CLUSTER MEMBERSHIP (argmax(u))\n",
    "        self.centers = cntr\n",
    "\n",
    "        # CREATE RANDOM FOREST AND TRAIN\n",
    "        self.rf = self.train_rf(attributes, model_perf, rf_controls=self.rf_kwargs)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, attributes):\n",
    "\n",
    "        # CHECK WHETHER MODEL HAS BEEN TRAINED\n",
    "        if self.rf is None:\n",
    "            raise(Exception(\"ModelSelector isn't trained!\"))\n",
    "\n",
    "        # GET RANDOM FOREST PREDICTION\n",
    "        pred_cluster_scores = self.rf.predict(attributes)\n",
    "\n",
    "        self.pred_cluster_scores = pred_cluster_scores # FOR TROUBLESHOOTING, DELETEME\n",
    "\n",
    "        # CALCULATE PROBABILITY OF INDIVIDUAL MODELS\n",
    "        prob_model = pred_cluster_scores * self.centers\n",
    "\n",
    "        self.prob_model = prob_model # FOR TROUBLESHOOTING, DELETEME\n",
    "\n",
    "        return prob_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CAMELS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\anaconda3\\envs\\SITest\\lib\\site-packages\\geopandas\\geodataframe.py:1483: FutureWarning: Passing 'suffixes' which cause duplicate columns {'gauge_id_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = DataFrame.merge(self, *args, **kwargs)\n",
      "c:\\Users\\franc\\anaconda3\\envs\\SITest\\lib\\site-packages\\geopandas\\geodataframe.py:1483: FutureWarning: Passing 'suffixes' which cause duplicate columns {'gauge_id_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = DataFrame.merge(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# FILEPATH TO SHAPEFILE CONTAINING CAMELS DATASET\n",
    "camelsdir = r\"C:\\Users\\franc\\OneDrive - University Of Houston\\000_SIResearch\\data\\HCDN_nhru_final\\HCDN_nhru_final_671.shp\"\n",
    "\n",
    "# DIRECTORY TO FOLDER CONTAINING CAMELS ATTRIBUTE TEXTFILES\n",
    "# PRIOR TO THIS STEP MAKE SURE THE README IN THE FILE SYSTEM HAS BEEN REMOVED (or the file extension has been changed)\n",
    "attdir = r\"C:\\Users\\franc\\OneDrive - University Of Houston\\000_SIResearch\\data\\camels_attributes_v2.0\\camels_attributes_v2.0\\\\\"\n",
    "\n",
    "# READ CAMELS DATASET\n",
    "camels = gpd.read_file(camelsdir)\n",
    "\n",
    "\n",
    "# COPY TO KEEP ORIGINAL IN MEMORY\n",
    "camels_df = camels \n",
    "\n",
    "# LOOP THROUGH AND JOIN\n",
    "filelist = glob(attdir + \"*.txt\")\n",
    "for i in filelist:\n",
    "    currdf = pd.read_csv(i, sep=\";\")\n",
    "    camels_df = camels_df.merge(currdf, how='left', left_on=\"hru_id\", right_on=\"gauge_id\")\n",
    "\n",
    "camels_df.head()\n",
    "\n",
    "# CLEAN UP NONSENSICAL DATA (EG, BASIN LABELS)\n",
    "# SO LETS GET A LIST OF VARIABLE NAMES WE WANT TO KEEP.\n",
    "\n",
    "# TO START WE WILL KEEP THE SAME VARIABLES AS Kratzert et al. 2019, AS SHOWN BY OUR\n",
    "# INTERNAL SPREADSHEET Attributes_CAMELS_vs_NHDPlus\n",
    "varstokeep = ['p_mean',\n",
    "'pet_mean',\n",
    "'aridity',\n",
    "'p_seasonality',\n",
    "'frac_snow',                   # In spreadsheet as 'frac_snow_daily'\n",
    "'high_prec_freq',\n",
    "'high_prec_dur',\n",
    "'low_prec_freq',\n",
    "'low_prec_dur',\n",
    "'elev_mean_x',                 # In spreadsheet as 'elev_mean' \n",
    "'slope_mean',\n",
    "'area_gages2',\n",
    "'frac_forest',                 # In spreadsheet as 'forest_frac'\n",
    "'lai_max',\n",
    "'lai_diff',\n",
    "'gvf_max',\n",
    "'gvf_diff',\n",
    "'soil_depth_pelletier',\n",
    "'soil_depth_statsgo',\n",
    "'soil_porosity',\n",
    "'soil_conductivity',\n",
    "'max_water_content',\n",
    "'sand_frac',\n",
    "'silt_frac',\n",
    "'clay_frac',\n",
    "'carbonate_rocks_frac',         # In spreadsheet as 'carb_rocks_frac'\n",
    "'geol_permeability']\n",
    "\n",
    "# DO NOT DELETE!!! ---------------------------------------------------------------------------------------------\n",
    "# CHECK OUR VALUES ARE GOOD\n",
    "tflist = []\n",
    "for i in varstokeep: # LOOP THROUGH AND CHECK COLUMN NAME IS IN DATAFRAME, STORE RESULT IN TFLIST\n",
    "    tflist.append(i in camels_df)\n",
    "\n",
    "# CONVERT TF LIST TO NUMPY ARRAY, THEN CHECK IF ALL ARE TRUE. IF NOT, PRINT WHICH ONE ISN'T AND RAISE EXCEPTION\n",
    "tflist = np.array(tflist)\n",
    "if np.any(np.logical_not(tflist)):\n",
    "    print(\"\\n\".join(np.array(varstokeep)[np.logical_not(tflist)]))\n",
    "    raise(Exception(\"Printed values not in CAMELS DataFrame\"))\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "inputdataset = camels_df[varstokeep]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: ONCE WE GET THE REAL PERFORMANCE METRICS, MODIFY ACCORDINGLY\n",
    "\n",
    "# Wrapper function for SKLearn MakeBlobs\n",
    "def generateRandomFit(size, models, std = 0.05, center = (0.1, 0.9), n = 5):\n",
    "    # Random state = 1 for reproducibility\n",
    "    X, y = make_blobs(n_samples=size, cluster_std=std, center_box=center, centers=n, n_features=models, random_state=1)\n",
    "    return X, y\n",
    "\n",
    "X, y = generateRandomFit(671, 3)\n",
    "\n",
    "outputdataset = X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[0.31996677 0.4028829  0.20233777]\n",
      " [0.37271176 0.33812005 0.23524379]]\n",
      "[[0.37115071 0.35500335 0.42366885]\n",
      " [0.35318508 0.71803877 0.12237367]]\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "test = ModelSelector()\n",
    "\n",
    "test.fit(inputdataset, outputdataset)\n",
    "\n",
    "\n",
    "print(test.n_centers)\n",
    "\n",
    "test_pred = test.predict(inputdataset.iloc[0:2])\n",
    "\n",
    "print(test.pred_cluster_scores)\n",
    "print(test.centers)\n",
    "print(test_pred.shape)\n",
    "\n",
    "float(np.nanmean((sim-obs)**2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d77c5c5e6a764b4b2b49233b218dfda5cf7d8153b08c1fb16e2a7068e0d8f037"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('SITest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
