{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESSENTIALS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CLUSTERING AND RANDOM FOREST\n",
    "import skfuzzy as fuzz\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# DATA LIBRARIES\n",
    "import geopandas as gpd\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# PREFERENCES\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our Scikit-Learn Compliant Class so we can implement validation schemes within. Only two methods required: predict() and fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelector():\n",
    "\n",
    "    def __init__(self, c_kwargs={}, rf_kwargs={}):\n",
    "       self.c_kwargs=c_kwargs        # CLUSTERING HYPERPARAMETERS\n",
    "       self.rf_kwargs=rf_kwargs      # RANDOM FOREST HYPERPARAMETERS\n",
    "       self.m = 2                    # EXPONENTIATION COEFFICIENT FOR CLUSTERING. TODO: MAKE ADJUSTABLE\n",
    "\n",
    "    def fuzzyCluster(self, data):\n",
    "        # Wraps Fuzzy Cluster function, only outputting percent belongs and formal cluster.\n",
    "\n",
    "        # CHECK THAT REQUIRED FIELDS ARE IN KWARGS, IF NOT ADD\n",
    "        if \"error\" not in self.c_kwargs:\n",
    "            self.c_kwargs['error']=0.005\n",
    "\n",
    "        if \"maxiter\" not in self.c_kwargs:\n",
    "            self.c_kwargs['maxiter']=1000\n",
    "\n",
    "        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(data.T, self.n_centers, self.m, **self.c_kwargs)\n",
    "        label = np.argmax(u, axis=0)\n",
    "        return cntr, u, fpc, label\n",
    "\n",
    "    def howManyClusters(self, X, mintest=2,maxtest=15):\n",
    "        # Determines how many clusters should be used using the Fuzzy Partitions Coefficient (FPC)\n",
    "        # https://scikit-fuzzy.github.io/scikit-fuzzy/auto_examples/plot_cmeans.html#example-plot-cmeans-py\n",
    "        # TODO: FIGURE OUT IF THIS METHOD IS APPROPRIATE OR NOT\n",
    "        fpcs = []\n",
    "        listtests = np.arange(mintest,maxtest)\n",
    "        for ncenters in listtests:\n",
    "            self.n_centers = ncenters\n",
    "            _, _, fpc, _ = self.fuzzyCluster(X)\n",
    "            fpcs.append(fpc)\n",
    "        return listtests[np.argmax(fpcs)]\n",
    "\n",
    "    def train_rf(self, X_train, y_train, rf_controls={}):\n",
    "        # ADAPTED FROM https://stackoverflow.com/questions/28489667/combining-random-forest-models-in-scikit-learn\n",
    "\n",
    "        # RF CONTROLS PASSED DIRECTLY FROM PARAMETER, DEFAULT IS EMPTY\n",
    "        rf = RandomForestRegressor(**rf_controls) \n",
    "\n",
    "        # RF FITTING \n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        return rf\n",
    "\n",
    "    def fit(self, attributes, model_perf):\n",
    "        # GET NUMBER OF CLUSTERS VIA MAXIMUM FUZZY PARTITION COEFFICIENT \n",
    "        self.n_centers = self.howManyClusters(model_perf)\n",
    "\n",
    "        # RUN CLUSTERING AND SAVE CENTERS FOR FUTURE PREDICTIONS\n",
    "        cntr, u, fpc, label = self.fuzzyCluster(model_perf)\n",
    "        # cntr:  CLUSTER CENTERS, WHICH ARE ON N-DIM MODEL PERFORMANCE SPACE\n",
    "        #        WHERE N IS THE NUMBER OF MODELS BEING COMPARED. \n",
    "        # u:     CLUSTER MEMBERSHIP MATRIX (% BELONGING)\n",
    "        # fpc:   FUZZY PARTITION COEFFICIENT FOR CLUSTERING RUN\n",
    "        # label: \"CLOSEST CLUSTER\", DEFINED BY MAXIMUM CLUSTER MEMBERSHIP (argmax(u))\n",
    "        self.centers = cntr\n",
    "\n",
    "        u = u.T # FLIP DIMENSIONS FOR U MATRIX\n",
    "\n",
    "        # CREATE RANDOM FOREST AND TRAIN\n",
    "        self.rf = self.train_rf(attributes, u, rf_controls=self.rf_kwargs)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, attributes):\n",
    "\n",
    "        # CHECK WHETHER MODEL HAS BEEN TRAINED\n",
    "        if self.rf is None:\n",
    "            raise(Exception(\"ModelSelector isn't trained!\"))\n",
    "\n",
    "        # GET RANDOM FOREST PREDICTION\n",
    "        pred_cluster_scores = self.rf.predict(attributes)\n",
    "\n",
    "        self.pred_cluster_scores = pred_cluster_scores # FOR TROUBLESHOOTING, DELETEME\n",
    "\n",
    "        # CALCULATE PROBABILITY OF INDIVIDUAL MODELS BY TAKING INTO ACCOUNT LIKELIHOOD THAT THEY ARE IN A CLUSTER (CLUSTER SCORES)\n",
    "        # AND PERCENT TIMES THAT A GIVEN MODEL IS BETTER IN A GIVEN CLUSTER (CENTERS)\n",
    "        prob_model = np.matmul(pred_cluster_scores, self.centers)\n",
    "\n",
    "        self.prob_model = prob_model # FOR TROUBLESHOOTING, DELETEME\n",
    "\n",
    "        return prob_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CAMELS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\anaconda3\\envs\\SITest\\lib\\site-packages\\geopandas\\geodataframe.py:1349: FutureWarning: Passing 'suffixes' which cause duplicate columns {'gauge_id_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = DataFrame.merge(self, *args, **kwargs)\n",
      "c:\\Users\\franc\\anaconda3\\envs\\SITest\\lib\\site-packages\\geopandas\\geodataframe.py:1349: FutureWarning: Passing 'suffixes' which cause duplicate columns {'gauge_id_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = DataFrame.merge(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# FILEPATH TO SHAPEFILE CONTAINING CAMELS DATASET\n",
    "camelsdir = r\"C:\\Users\\franc\\OneDrive - University Of Houston\\000_SIResearch\\data\\HCDN_nhru_final\\HCDN_nhru_final_671.shp\"\n",
    "\n",
    "# DIRECTORY TO FOLDER CONTAINING CAMELS ATTRIBUTE TEXTFILES\n",
    "# PRIOR TO THIS STEP MAKE SURE THE README IN THE FILE SYSTEM HAS BEEN REMOVED (or the file extension has been changed)\n",
    "attdir = r\"C:\\Users\\franc\\OneDrive - University Of Houston\\000_SIResearch\\data\\camels_attributes_v2.0\\camels_attributes_v2.0\\\\\"\n",
    "\n",
    "# READ CAMELS DATASET\n",
    "camels = gpd.read_file(camelsdir)\n",
    "\n",
    "\n",
    "# COPY TO KEEP ORIGINAL IN MEMORY\n",
    "camels_df = camels \n",
    "\n",
    "# LOOP THROUGH AND JOIN\n",
    "filelist = glob(attdir + \"*.txt\")\n",
    "for i in filelist:\n",
    "    currdf = pd.read_csv(i, sep=\";\")\n",
    "    camels_df = camels_df.merge(currdf, how='left', left_on=\"hru_id\", right_on=\"gauge_id\")\n",
    "\n",
    "# DEFINE WHAT WE WANT TO RUN ON\n",
    "perf_dir = r\"C:\\Users\\franc\\OneDrive - University Of Houston\\000_SIResearch\\Repo\\nextgen-form-eval\\FrankTests\\data\\JonathanTests\\\\\"\n",
    "perf_prefixes = [\"daymet_time_split1\", \"daymet_time_split2\", \"nldas_time_split1\", \"nldas_time_split2\"]\n",
    "perf_prefixes_abb = [\"daymetS1\", \"daymetS2\", \"nldasS1\", \"nldasS2\"]\n",
    "perf_metrics = [\"KGE\"]\n",
    "\n",
    "# CONTAINER FOR PERFORMANCE VARIABLE NAMES\n",
    "perfvars = list()\n",
    "perfvars_split = list()\n",
    "\n",
    "\n",
    "# LOOP THROUGH EACH TO ADD TO CAMELS DATASET AND SAVE COLUMN NAME\n",
    "for i in range(0, len(perf_prefixes)):\n",
    "    for ii in range(0, len(perf_metrics)):\n",
    "        currdir = perf_dir + perf_prefixes[i] + \"_\" + perf_metrics[ii] + \".csv\"\n",
    "        currdf = pd.read_csv(currdir).add_prefix(perf_prefixes_abb[i] + \"_\" + perf_metrics[ii] + \"_\")\n",
    "        first_col_name = currdf.columns.to_list()[0]\n",
    "        perfvars.extend(currdf.columns.to_list()[1:])\n",
    "        perfvars_split.append(currdf.columns.to_list()[1:])\n",
    "        camels_df = camels_df.merge(currdf, how='right', left_on=\"hru_id\", right_on=first_col_name)\n",
    "\n",
    "\n",
    "# CLEAN UP NONSENSICAL DATA (EG, BASIN LABELS)\n",
    "# SO LETS GET A LIST OF VARIABLE NAMES WE WANT TO KEEP.\n",
    "\n",
    "# TO START WE WILL KEEP THE SAME VARIABLES AS Kratzert et al. 2019, AS SHOWN BY OUR\n",
    "# INTERNAL SPREADSHEET Attributes_CAMELS_vs_NHDPlus\n",
    "varstokeep = ['p_mean',\n",
    "'pet_mean',\n",
    "'aridity',\n",
    "'p_seasonality',\n",
    "'frac_snow',                   # In spreadsheet as 'frac_snow_daily'\n",
    "'high_prec_freq',\n",
    "'high_prec_dur',\n",
    "'low_prec_freq',\n",
    "'low_prec_dur',\n",
    "'elev_mean_x',                 # In spreadsheet as 'elev_mean' \n",
    "'slope_mean',\n",
    "'area_gages2',\n",
    "'frac_forest',                 # In spreadsheet as 'forest_frac'\n",
    "'lai_max',\n",
    "'lai_diff',\n",
    "'gvf_max',\n",
    "'gvf_diff',\n",
    "'soil_depth_pelletier',\n",
    "'soil_depth_statsgo',\n",
    "'soil_porosity',\n",
    "'soil_conductivity',\n",
    "'max_water_content',\n",
    "'sand_frac',\n",
    "'silt_frac',\n",
    "'clay_frac',\n",
    "'carbonate_rocks_frac',         # In spreadsheet as 'carb_rocks_frac'\n",
    "'geol_permeability']\n",
    "\n",
    "# DO NOT DELETE!!! ---------------------------------------------------------------------------------------------\n",
    "# CHECK OUR VALUES ARE GOOD\n",
    "tflist = []\n",
    "for i in varstokeep: # LOOP THROUGH AND CHECK COLUMN NAME IS IN DATAFRAME, STORE RESULT IN TFLIST\n",
    "    tflist.append(i in camels_df)\n",
    "\n",
    "# CONVERT TF LIST TO NUMPY ARRAY, THEN CHECK IF ALL ARE TRUE. IF NOT, PRINT WHICH ONE ISN'T AND RAISE EXCEPTION\n",
    "tflist = np.array(tflist)\n",
    "if np.any(np.logical_not(tflist)):\n",
    "    print(\"\\n\".join(np.array(varstokeep)[np.logical_not(tflist)]))\n",
    "    raise(Exception(\"Printed values not in CAMELS DataFrame\"))\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "inputdataset = camels_df[varstokeep]\n",
    "outputdataset = camels_df[perfvars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daymet_time_split1\n",
      "0.02328261379932352\n",
      "0.01825352892230141\n",
      "0.015223049983200006\n",
      "0.03676709261107749\n",
      "0.0309076246248481\n",
      "daymet_time_split2\n",
      "0.021373605258684477\n",
      "0.0239495866022314\n",
      "0.01745226925681328\n",
      "0.034693785692854716\n",
      "0.027596692665437045\n",
      "nldas_time_split1\n",
      "0.02773769875573451\n",
      "0.056818640711844226\n",
      "0.03438746042365094\n",
      "0.023276106591454956\n",
      "0.02187606217309108\n",
      "nldas_time_split2\n",
      "0.050954012609538565\n",
      "0.024362366313245152\n",
      "0.03562863910857439\n",
      "0.03290549306464967\n",
      "0.0187464439986079\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for i in range(0, len(perf_prefixes)):\n",
    "    currout = outputdataset[perfvars_split[i]]\n",
    "    print(perf_prefixes[i])\n",
    "    for train, test in kf.split(inputdataset):\n",
    "        model = ModelSelector()\n",
    "        model.fit(inputdataset.iloc[train, :], currout.iloc[train, :])\n",
    "        model_pred = model.predict(inputdataset.iloc[test, :])\n",
    "        print(np.mean((model_pred - currout.iloc[test, :].to_numpy())**2))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d77c5c5e6a764b4b2b49233b218dfda5cf7d8153b08c1fb16e2a7068e0d8f037"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('SITest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
